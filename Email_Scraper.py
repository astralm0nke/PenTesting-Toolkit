from bs4 import BeautifulSoup
import requests, re, lxml
from requests import exceptions
from urllib import parse
from collections import deque
##----------------EMAIL SCRAPPER----------------##
user_url = str(input('[+] Specify Target URL to Scan: '))
user_count = int(input('[+] Specify Count Limit: '))
urls = deque([user_url])

scrapped_urls = set()
emails = set()

count = 0
try:
    while len(urls):
        count += 1
        if count == (user_count+1):
            break
        url = urls.popleft()
        scrapped_urls.add(url)
        
        parts = parse.urlsplit(url)
        base_url = '{0.scheme}://{0.netloc}'.format(parts)
        path = url[:url.rfind('/')+1] if '/' in parts.path else url
        
        print('[%d] Processing %s' % (count, url))
        
        try:
            response = requests.get(url)
        except (exceptions.MissingSchema, exceptions.ConnectionError):
            continue
        
        new_emails = set(re.findall(r'[a-z0-9\.\-+_]+@[a-z0-9\.\-+_]+\.[a-z]+', response.text, re.I))
        emails.update(new_emails)
        
        soup = BeautifulSoup(response.text, features='lxml')
        
        for anchor in soup.find_all('a'):
            link = anchor.attrs['href'] if 'href' in anchor.attrs else ''
            if link.startswith('/'):
                link = base_url + link
            elif not link.startswith('http'):
                link = path + link
            if not link in urls and not link in scrapped_urls:
                urls.append(link)
except KeyboardInterrupt:
    print('[-] Closing Program')
    
if (emails):
    print('[!] Emails Found!')
    for mail in (emails):
        print(mail)
else:
    print('[-] No Emails Found. Womp-Womp!')